{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on performance, the CPU & GPU, and composability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is shipped with OpenBLAS by default, and many linalg operations routed to it; in particular gemm, or mul! in julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "] instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.14279022693338"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with a single threaded m=n=k=4000 matrix multiply:\n",
    "BLAS.set_num_threads(1)\n",
    "single_thread_gflops = peakflops(4000) / 10^9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ Machine (63GB)                                                                                                                                                                                                 │\n",
      "│                                                                                                                                                                                                                │\n",
      "│ ┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │\n",
      "│ │ Package P#0                                                                                                                                                                                                │ │\n",
      "│ │                                                                                                                                                                                                            │ │\n",
      "│ │ ┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ │\n",
      "│ │ │ L3 (30MB)                                                                                                                                                                                              │ │ │\n",
      "│ │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ │ │\n",
      "│ │                                                                                                                                                                                                            │ │\n",
      "│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │\n",
      "│ │ │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │  │ L2 (256KB)  │ │ │\n",
      "│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │\n",
      "│ │                                                                                                                                                                                                            │ │\n",
      "│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │\n",
      "│ │ │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │  │ L1d (32KB)  │ │ │\n",
      "│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │\n",
      "│ │                                                                                                                                                                                                            │ │\n",
      "│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │\n",
      "│ │ │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │  │ L1i (32KB)  │ │ │\n",
      "│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │\n",
      "│ │                                                                                                                                                                                                            │ │\n",
      "│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │\n",
      "│ │ │ Core P#0    │  │ Core P#1    │  │ Core P#2    │  │ Core P#3    │  │ Core P#4    │  │ Core P#5    │  │ Core P#8    │  │ Core P#9    │  │ Core P#10   │  │ Core P#11   │  │ Core P#12   │  │ Core P#13   │ │ │\n",
      "│ │ │             │  │             │  │             │  │             │  │             │  │             │  │             │  │             │  │             │  │             │  │             │  │             │ │ │\n",
      "│ │ │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │ │ │\n",
      "│ │ │ │ PU P#0  │ │  │ │ PU P#1  │ │  │ │ PU P#2  │ │  │ │ PU P#3  │ │  │ │ PU P#4  │ │  │ │ PU P#5  │ │  │ │ PU P#6  │ │  │ │ PU P#7  │ │  │ │ PU P#8  │ │  │ │ PU P#9  │ │  │ │ PU P#10 │ │  │ │ PU P#11 │ │ │ │\n",
      "│ │ │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │ │ │\n",
      "│ │ │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │  │ ┌─────────┐ │ │ │\n",
      "│ │ │ │ PU P#12 │ │  │ │ PU P#13 │ │  │ │ PU P#14 │ │  │ │ PU P#15 │ │  │ │ PU P#16 │ │  │ │ PU P#17 │ │  │ │ PU P#18 │ │  │ │ PU P#19 │ │  │ │ PU P#20 │ │  │ │ PU P#21 │ │  │ │ PU P#22 │ │  │ │ PU P#23 │ │ │ │\n",
      "│ │ │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │  │ └─────────┘ │ │ │\n",
      "│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │\n",
      "│ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ │\n",
      "│                                                                                                                                                                                                                │\n",
      "│                     ┌───────────────┐                                                                                                                                                                          │\n",
      "│ ├┤╶───────┼┤╶───────┤ PCI 10de:15f8 │                                                                                                                                                                          │\n",
      "│                     └───────────────┘                                                                                                                                                                          │\n",
      "└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ Host: nid07116                                                                                                                                                                                                 │\n",
      "│                                                                                                                                                                                                                │\n",
      "│ Indexes: physical                                                                                                                                                                                              │\n",
      "│                                                                                                                                                                                                                │\n",
      "│ Date: Sun 25 Apr 2021 10:26:46 PM CEST                                                                                                                                                                         │\n",
      "└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# What are we actually on? Call system `lstopo` (There's also Hwloc.jl, of course)\n",
    "run(`lstopo --of txt`);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325.81507911080297"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maybe someone at CSCS will be disappointed with these numbers\n",
    "# since it's using OpenBLAS, not MKL. But then again, don't we want\n",
    "# to live in a world where we use open software?!\n",
    "BLAS.set_num_threads(12)\n",
    "multi_gflops = peakflops(4000) / 10^9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clearly OpenBLAS is not state-of-the art on Intel hardware, it doesn't scale well with more cores :(\n",
    "\n",
    "round(multi_gflops / single_thread_gflops, digits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Julia GEMM for the CPU\n",
    "\n",
    "Can we get anything better than OpenBLAS? Yes! In fact we can get:\n",
    "\n",
    "- any BLAS (license permitting...) through the BinaryBuilder project (MKL, BLIS, ...)\n",
    "- a GEMM we're writing ourselves in Julia -- but standing on the shoulders of a loop vectorization package written in ... Julia\n",
    "\n",
    "As soon as Julia 1.7 is release, [libblastrampoline](https://github.com/staticfloat/libblastrampoline) will be the default. It's light wrapper for BLAS that dispatches to whatever you have installed; very convenient! Also in the C, C++ and Python (?) world!\n",
    "\n",
    "NOTE! If you want to use Julia's builtin threading (just threads or task-based parallellism), make sure it is actually enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JULIA_NUM_THREADS=12\n",
      "export JULIA_PKG_DEVDIR=\"$SCRATCH/julia_dev\"\n"
     ]
    }
   ],
   "source": [
    "# I've created this by hand to make my life easier when doing threaded Julia code -- maybe restart your kernel if you haven't added this already\n",
    "`cat $(ENV[\"HOME\"])/.jupyterhub.env` |> run;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out MKL first and see if we get to the peakflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BLASBenchmarksCPU: gemmmkl!, mkl_set_num_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkl_set_num_threads(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peakgflops (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function peakgflops(f!, n)\n",
    "    A = rand(n, n); B = rand(n, n); C = rand(n, n)\n",
    "    sec = @elapsed f!(C, A, B)\n",
    "    return 2n^3 / sec / 10^9\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375.6812956449564"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peakgflops(gemmmkl!, 8_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mygemm!"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LoopVectorization\n",
    "\n",
    "\"\"\"\n",
    "Textbook matrix multiplication\n",
    "\n",
    "The @avxt macro is used to enable LoopVectorization's ... loop vectorization\n",
    "which supports nested loops. Note that this is generally unsafe! No bounds\n",
    "checks and reordering of operations are allowed.\n",
    "\"\"\"\n",
    "function mygemm!(C, A, B)\n",
    "    @avxt for m ∈ axes(A,1), n ∈ axes(B,2)\n",
    "        Cmn = zero(eltype(C))\n",
    "        for k ∈ axes(A,2)\n",
    "           Cmn += A[m,k] * B[k,n]\n",
    "        end\n",
    "        C[m,n] = Cmn\n",
    "    end\n",
    "    \n",
    "    return C\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m, \u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(10, 10)\n",
    "B = rand(10, 10)\n",
    "C = rand(10, 10)\n",
    "\n",
    "using Test\n",
    "\n",
    "# make sure not to reuse an existing output array in the test :D; so copy(A)\n",
    "function verify(f!, A, B)\n",
    "    result = f!(copy(A), A, B)\n",
    "    expected = A * B\n",
    "    norm(result - expected, Inf) < 10eps()\n",
    "end\n",
    "\n",
    "@test(verify(mygemm!, A, B)), @test(verify(gemmmkl!, A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "small_matrix_bench (generic function with 3 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, CUDA, UnicodePlots\n",
    "\n",
    "function small_matrix_bench(n, ::Type{T} = Float64, peak = 2.6e9 #=Hz=# * 12 #=cores=# * 4 #=Float64/register=# * 4 #=units=#) where {T}\n",
    "    # Allocate some matrices (cpu, gpu)\n",
    "    A = rand(T, n, n); B = rand(T, n, n); C = rand(T, n, n)\n",
    "    A_d = CUDA.rand(T, n, n); B_d = CUDA.rand(T, n, n); C_d = CUDA.rand(T, n, n)\n",
    "    \n",
    "    # Run benchmarks (will do warmup etc etc too)\n",
    "    julia    = @belapsed mygemm!($C, $A, $B)\n",
    "    mkl      = @belapsed gemmmkl!($C, $A, $B)\n",
    "    cuda     = @belapsed CUDA.@sync mul!($C_d, $A_d, $B_d)\n",
    "    \n",
    "    # Show results\n",
    "    julia_perc_of_peak = round((2n^3 / julia) / peak * 100, digits=2)\n",
    "    mkl_perc_of_peak   = round((2n^3 / mkl)   / peak * 100, digits=2)\n",
    "    cuda_perc_of_peak  = round((2n^3 / cuda)  / peak * 100, digits=2)\n",
    "    \n",
    "    println(barplot(\n",
    "        [\"Julia\", \"MKL\", \"cuBLAS\"],\n",
    "        [julia_perc_of_peak, mkl_perc_of_peak, cuda_perc_of_peak],\n",
    "        title=\"Size $n x $n\",\n",
    "        xlabel=\"% of peak CPU performance\"\n",
    "    ), \"\\n\")\n",
    "    \n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                        Size 32 x 32\u001b[22m\n",
      "\u001b[90m          ┌                                        ┐\u001b[39m \n",
      "    \u001b[0mJulia\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 15.09 \u001b[90m \u001b[39m \n",
      "      \u001b[0mMKL\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 6.97                    \u001b[90m \u001b[39m \n",
      "   \u001b[0mcuBLAS\u001b[90m ┤\u001b[39m\u001b[32m■■\u001b[39m\u001b[0m 0.82                                 \u001b[90m \u001b[39m \n",
      "\u001b[90m          └                                        ┘\u001b[39m \n",
      "\u001b[0m                  % of peak CPU performance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_matrix_bench(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                        Size 33 x 33\u001b[22m\n",
      "\u001b[90m          ┌                                        ┐\u001b[39m \n",
      "    \u001b[0mJulia\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 13.52 \u001b[90m \u001b[39m \n",
      "      \u001b[0mMKL\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■\u001b[39m\u001b[0m 4.82                       \u001b[90m \u001b[39m \n",
      "   \u001b[0mcuBLAS\u001b[90m ┤\u001b[39m\u001b[32m■■\u001b[39m\u001b[0m 0.8                                  \u001b[90m \u001b[39m \n",
      "\u001b[90m          └                                        ┘\u001b[39m \n",
      "\u001b[0m                  % of peak CPU performance\n",
      "\n",
      "\u001b[1m                        Size 65 x 65\u001b[22m\n",
      "\u001b[90m          ┌                                        ┐\u001b[39m \n",
      "    \u001b[0mJulia\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 31.63 \u001b[90m \u001b[39m \n",
      "      \u001b[0mMKL\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 29.53   \u001b[90m \u001b[39m \n",
      "   \u001b[0mcuBLAS\u001b[90m ┤\u001b[39m\u001b[32m■■■■■\u001b[39m\u001b[0m 5.27                              \u001b[90m \u001b[39m \n",
      "\u001b[90m          └                                        ┘\u001b[39m \n",
      "\u001b[0m                  % of peak CPU performance\n",
      "\n",
      "\u001b[1m                        Size 97 x 97\u001b[22m\n",
      "\u001b[90m          ┌                                        ┐\u001b[39m \n",
      "    \u001b[0mJulia\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 60.57 \u001b[90m \u001b[39m \n",
      "      \u001b[0mMKL\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 39.3              \u001b[90m \u001b[39m \n",
      "   \u001b[0mcuBLAS\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■\u001b[39m\u001b[0m 15.0                           \u001b[90m \u001b[39m \n",
      "\u001b[90m          └                                        ┘\u001b[39m \n",
      "\u001b[0m                  % of peak CPU performance\n",
      "\n",
      "\u001b[1m                       Size 129 x 129\u001b[22m\n",
      "\u001b[90m          ┌                                        ┐\u001b[39m \n",
      "    \u001b[0mJulia\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 64.56 \u001b[90m \u001b[39m \n",
      "      \u001b[0mMKL\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 49.55         \u001b[90m \u001b[39m \n",
      "   \u001b[0mcuBLAS\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 30.24                   \u001b[90m \u001b[39m \n",
      "\u001b[90m          └                                        ┘\u001b[39m \n",
      "\u001b[0m                  % of peak CPU performance\n",
      "\n",
      "\u001b[1m                       Size 161 x 161\u001b[22m\n",
      "\u001b[90m          ┌                                        ┐\u001b[39m \n",
      "    \u001b[0mJulia\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 71.22 \u001b[90m \u001b[39m \n",
      "      \u001b[0mMKL\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 57.91       \u001b[90m \u001b[39m \n",
      "   \u001b[0mcuBLAS\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 50.95          \u001b[90m \u001b[39m \n",
      "\u001b[90m          └                                        ┘\u001b[39m \n",
      "\u001b[0m                  % of peak CPU performance\n",
      "\n",
      "\u001b[1m                       Size 193 x 193\u001b[22m\n",
      "\u001b[90m          ┌                                        ┐\u001b[39m \n",
      "    \u001b[0mJulia\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 59.23         \u001b[90m \u001b[39m \n",
      "      \u001b[0mMKL\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 52.65            \u001b[90m \u001b[39m \n",
      "   \u001b[0mcuBLAS\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 77.68 \u001b[90m \u001b[39m \n",
      "\u001b[90m          └                                        ┘\u001b[39m \n",
      "\u001b[0m                  % of peak CPU performance\n",
      "\n",
      "\u001b[1m                       Size 225 x 225\u001b[22m\n",
      "\u001b[90m          ┌                                        ┐\u001b[39m \n",
      "    \u001b[0mJulia\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 57.94                 \u001b[90m \u001b[39m \n",
      "      \u001b[0mMKL\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 57.52                 \u001b[90m \u001b[39m \n",
      "   \u001b[0mcuBLAS\u001b[90m ┤\u001b[39m\u001b[32m■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 106.98 \u001b[90m \u001b[39m \n",
      "\u001b[90m          └                                        ┘\u001b[39m \n",
      "\u001b[0m                  % of peak CPU performance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test some \"awkward\" sizes for vectorization -- found in real life too\n",
    "foreach(small_matrix_bench, 33:32:225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this sparks your interest, read these resources, they are really nice with many plots: https://juliasimd.github.io/LoopVectorization.jl/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about CUDA?\n",
    "\n",
    "Clearly at modest sizes already the GPU is where you want to run your computations. But GPU programming is hard, right?\n",
    "\n",
    "Julia makes it incredibly easy to write generic CUDA kernels. CUDA.jl is the most mature GPU programming package -- AMDGPU.jl is out there too, and they share a lot of code; device-agnostic, generic GPU programming is getting there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a generic CUDA kernel that ±saturates the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ] add CUDA -- this is the package you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cudapeakflops"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "\n",
    "\"\"\"\n",
    "Dummy kernel doing 33 * 6 + 3 = 201 FMAs.\n",
    "Should get us reasonably close to peak performance\n",
    "Adapted from Tim Besard's example in CUDA.jl\n",
    "\"\"\"\n",
    "function kernel_201_fma(a, b, c, d, e, f, out)\n",
    "    # side-note: the types of the kernel args will be CuDeviceArray, which\n",
    "    # is multidimensional array type supporting cartesian indexing;\n",
    "    # but are doing scalar operations, so stick to fast linear indexing.\n",
    "\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "\n",
    "    @inbounds if i ≤ length(out)\n",
    "        aᵢ, bᵢ, cᵢ = a[i], b[i], c[i]\n",
    "        dᵢ, eᵢ, fᵢ = d[i], e[i], f[i]\n",
    "\n",
    "        for j = 1:33\n",
    "            aᵢ′ = fma(aᵢ, bᵢ, cᵢ)\n",
    "            bᵢ′ = fma(dᵢ, eᵢ, fᵢ)\n",
    "            cᵢ′ = fma(cᵢ, bᵢ, aᵢ)\n",
    "            dᵢ′ = fma(fᵢ, eᵢ, dᵢ)\n",
    "            eᵢ′ = fma(aᵢ, cᵢ, eᵢ)\n",
    "            fᵢ′ = fma(bᵢ, dᵢ, fᵢ)\n",
    "            \n",
    "            aᵢ, bᵢ, cᵢ = aᵢ′, bᵢ′, cᵢ′\n",
    "            dᵢ, eᵢ, fᵢ = dᵢ′, eᵢ′, fᵢ′\n",
    "        end\n",
    "\n",
    "        out₁ = fma(aᵢ, bᵢ, cᵢ)\n",
    "        out₂ = fma(dᵢ, eᵢ, fᵢ)\n",
    "        out[i] = fma(out₁, out₂, out₂)\n",
    "    end\n",
    "    \n",
    "    return\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Some generic benchmarking for number type T, applying\n",
    "the kernel to n×n matrices.\n",
    "\"\"\"\n",
    "function cudapeakflops(::Type{T} = Float64, n = 5000) where {T}\n",
    "    dims = (n, n)\n",
    "\n",
    "    # Create 6 input arrays with random data\n",
    "    # For Float32 / Float64 we can generate random numbers\n",
    "    # directly on the device.\n",
    "    \n",
    "    # For the C++ folks: in a language where the line\n",
    "    # between compile & runtime is blurred, there is\n",
    "    # no need for `if *constexpr* T == Float64` 🙃\n",
    "    # On a side-node: you have the freedom to use\n",
    "    # emoji's as variable names in generic CUDA kernels\n",
    "    # when you think that improves readability!\n",
    "    if T === Float64 || T === Float32\n",
    "        d_a = CUDA.rand(T, dims)\n",
    "        d_b = CUDA.rand(T, dims)\n",
    "        d_c = CUDA.rand(T, dims)\n",
    "        d_d = CUDA.rand(T, dims)\n",
    "        d_e = CUDA.rand(T, dims)\n",
    "        d_f = CUDA.rand(T, dims)\n",
    "    else\n",
    "        d_a = CuArray(rand(T, dims))\n",
    "        d_b = CuArray(rand(T, dims))\n",
    "        d_c = CuArray(rand(T, dims))\n",
    "        d_d = CuArray(rand(T, dims))\n",
    "        d_e = CuArray(rand(T, dims))\n",
    "        d_f = CuArray(rand(T, dims))\n",
    "    end\n",
    "\n",
    "    d_out = similar(d_a)\n",
    "\n",
    "    len = prod(dims)\n",
    "\n",
    "    # Create a kernel\n",
    "    kernel = @cuda launch=false kernel_201_fma(d_a, d_b, d_c, d_d, d_e, d_f, d_out)\n",
    "    config = launch_configuration(kernel.fun)\n",
    "    threads = Base.min(len, config.threads)\n",
    "    blocks = cld(len, threads)\n",
    "    \n",
    "    # Warm-up\n",
    "    kernel(d_a, d_b, d_c, d_d, d_e, d_f, d_out)\n",
    "    synchronize()\n",
    "\n",
    "    secs = CUDA.@elapsed begin\n",
    "        kernel(d_a, d_b, d_c, d_d, d_e, d_f, d_out; threads=threads, blocks=blocks)\n",
    "    end\n",
    "\n",
    "    # 201 fma's per kernel, 1 fma = 2 flops\n",
    "    flopcount = 2 * 201 * len\n",
    "\n",
    "    return round(flopcount / secs / 1e9, digits=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3740.67 GFlops\n"
     ]
    }
   ],
   "source": [
    "println(cudapeakflops(Float64), \" GFlops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6629.15 GFlops\n"
     ]
    }
   ],
   "source": [
    "println(cudapeakflops(Float32), \" GFlops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "another_peakflops (generic function with 1 method)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra: mul!\n",
    "\n",
    "function another_peakflops(::Type{T}, n) where {T}\n",
    "    A = CUDA.rand(T, n, n)\n",
    "    B = CUDA.rand(T, n, n)\n",
    "    C = CUDA.rand(T, n, n)\n",
    "    \n",
    "    sec = CUDA.@elapsed mul!(C, A, B)\n",
    "    \n",
    "    round(2n^3 / sec / 1e9, digits=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4293.17 GFlops\n"
     ]
    }
   ],
   "source": [
    "println(another_peakflops(Float64, 5000), \" GFlops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2636.76 GFlops\n"
     ]
    }
   ],
   "source": [
    "println(another_peakflops(Float32, 5000), \" GFlops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// PTX CompilerJob of kernel kernel_201_fma(CuDeviceMatrix{Float64, 1}, CuDeviceMatrix{Float64, 1}, CuDeviceMatrix{Float64, 1}, CuDeviceMatrix{Float64, 1}, CuDeviceMatrix{Float64, 1}, CuDeviceMatrix{Float64, 1}, CuDeviceMatrix{Float64, 1}) for sm_60\n",
      "\n",
      "//\n",
      "// Generated by LLVM NVPTX Back-End\n",
      "//\n",
      "\n",
      ".version 6.3\n",
      ".target sm_60\n",
      ".address_size 64\n",
      "\n",
      "\t// .globl\t_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE // -- Begin function _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE\n",
      "                                        // @_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE\n",
      ".visible .entry _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE(\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_0[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_1[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_2[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_3[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_4[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_5[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_6[24]\n",
      ")\n",
      "{\n",
      "\t.reg .pred \t%p<3>;\n",
      "\t.reg .b32 \t%r<5>;\n",
      "\t.reg .f64 \t%fd<40>;\n",
      "\t.reg .b64 \t%rd<42>;\n",
      "\n",
      "// %bb.0:                               // %entry\n",
      "\tmov.u32 \t%r2, %ctaid.x;\n",
      "\tld.param.u64 \t%rd24, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_6+8];\n",
      "\tld.param.u64 \t%rd25, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_6];\n",
      "\tmov.u32 \t%r3, %ntid.x;\n",
      "\tmul.wide.u32 \t%rd2, %r3, %r2;\n",
      "\tmov.u32 \t%r1, %tid.x;\n",
      "\tadd.s32 \t%r4, %r1, 1;\n",
      "\tcvt.u64.u32 \t%rd26, %r4;\n",
      "\tadd.s64 \t%rd27, %rd2, %rd26;\n",
      "\tmul.lo.s64 \t%rd28, %rd25, %rd24;\n",
      "\tsetp.gt.s64 \t%p1, %rd27, %rd28;\n",
      "\t@%p1 bra \tLBB0_4;\n",
      "// %bb.1:                               // %L42.i\n",
      "\tld.param.u64 \t%rd23, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_5+16];\n",
      "\tld.param.u64 \t%rd20, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_4+16];\n",
      "\tld.param.u64 \t%rd17, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_3+16];\n",
      "\tld.param.u64 \t%rd14, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_2+16];\n",
      "\tld.param.u64 \t%rd11, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_1+16];\n",
      "\tld.param.u64 \t%rd8, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_0+16];\n",
      "\tld.param.u64 \t%rd1, [_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_6+16];\n",
      "\tcvt.u64.u32 \t%rd30, %r1;\n",
      "\tadd.s64 \t%rd31, %rd2, %rd30;\n",
      "\tadd.s64 \t%rd3, %rd31, -1;\n",
      "\tshl.b64 \t%rd32, %rd3, 3;\n",
      "\tadd.s64 \t%rd33, %rd8, %rd32;\n",
      "\tld.global.f64 \t%fd39, [%rd33+8];\n",
      "\tadd.s64 \t%rd34, %rd11, %rd32;\n",
      "\tld.global.f64 \t%fd38, [%rd34+8];\n",
      "\tadd.s64 \t%rd35, %rd14, %rd32;\n",
      "\tld.global.f64 \t%fd37, [%rd35+8];\n",
      "\tadd.s64 \t%rd36, %rd17, %rd32;\n",
      "\tld.global.f64 \t%fd36, [%rd36+8];\n",
      "\tadd.s64 \t%rd37, %rd20, %rd32;\n",
      "\tld.global.f64 \t%fd35, [%rd37+8];\n",
      "\tadd.s64 \t%rd38, %rd23, %rd32;\n",
      "\tld.global.f64 \t%fd34, [%rd38+8];\n",
      "\tmov.u64 \t%rd41, 33;\n",
      "LBB0_2:                                 // %L181.i\n",
      "                                        // =>This Inner Loop Header: Depth=1\n",
      "\tfma.rn.f64 \t%fd19, %fd39, %fd38, %fd37;\n",
      "\tfma.rn.f64 \t%fd20, %fd36, %fd35, %fd34;\n",
      "\tfma.rn.f64 \t%fd21, %fd37, %fd38, %fd39;\n",
      "\tfma.rn.f64 \t%fd22, %fd34, %fd35, %fd36;\n",
      "\tfma.rn.f64 \t%fd23, %fd39, %fd37, %fd35;\n",
      "\tfma.rn.f64 \t%fd24, %fd38, %fd36, %fd34;\n",
      "\tfma.rn.f64 \t%fd25, %fd19, %fd20, %fd21;\n",
      "\tfma.rn.f64 \t%fd26, %fd22, %fd23, %fd24;\n",
      "\tfma.rn.f64 \t%fd27, %fd21, %fd20, %fd19;\n",
      "\tfma.rn.f64 \t%fd28, %fd24, %fd23, %fd22;\n",
      "\tfma.rn.f64 \t%fd29, %fd19, %fd21, %fd23;\n",
      "\tfma.rn.f64 \t%fd30, %fd20, %fd22, %fd24;\n",
      "\tfma.rn.f64 \t%fd39, %fd25, %fd26, %fd27;\n",
      "\tfma.rn.f64 \t%fd38, %fd28, %fd29, %fd30;\n",
      "\tfma.rn.f64 \t%fd37, %fd27, %fd26, %fd25;\n",
      "\tfma.rn.f64 \t%fd36, %fd30, %fd29, %fd28;\n",
      "\tfma.rn.f64 \t%fd35, %fd25, %fd27, %fd29;\n",
      "\tfma.rn.f64 \t%fd34, %fd26, %fd28, %fd30;\n",
      "\tadd.s64 \t%rd41, %rd41, -3;\n",
      "\tsetp.ne.s64 \t%p2, %rd41, 0;\n",
      "\t@%p2 bra \tLBB0_2;\n",
      "// %bb.3:                               // %L199.L204_crit_edge.i\n",
      "\tfma.rn.f64 \t%fd31, %fd39, %fd38, %fd37;\n",
      "\tfma.rn.f64 \t%fd32, %fd36, %fd35, %fd34;\n",
      "\tfma.rn.f64 \t%fd33, %fd31, %fd32, %fd32;\n",
      "\tadd.s64 \t%rd40, %rd1, %rd32;\n",
      "\tst.global.f64 \t[%rd40+8], %fd33;\n",
      "LBB0_4:                                 // %_Z26julia_kernel_201_fma_1146713CuDeviceArrayI7Float64Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE.inner.exit\n",
      "\tret;\n",
      "                                        // -- End function\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "CUDA.@device_code_ptx cudapeakflops(Float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// PTX CompilerJob of kernel kernel_201_fma(CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}) for sm_60\n",
      "\n",
      "//\n",
      "// Generated by LLVM NVPTX Back-End\n",
      "//\n",
      "\n",
      ".version 6.3\n",
      ".target sm_60\n",
      ".address_size 64\n",
      "\n",
      "\t// .globl\t_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE // -- Begin function _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE\n",
      ".global .align 1 .b8 $str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90, 0};\n",
      "                                        // @_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE\n",
      ".visible .entry _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE(\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_0[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_1[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_2[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_3[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_4[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_5[24],\n",
      "\t.param .align 8 .b8 _Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_6[24]\n",
      ")\n",
      "{\n",
      "\t.reg .pred \t%p<3>;\n",
      "\t.reg .f32 \t%f<28>;\n",
      "\t.reg .b32 \t%r<5>;\n",
      "\t.reg .b64 \t%rd<42>;\n",
      "\n",
      "// %bb.0:                               // %entry\n",
      "\tmov.u32 \t%r2, %ctaid.x;\n",
      "\tld.param.u64 \t%rd24, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_6+8];\n",
      "\tld.param.u64 \t%rd25, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_6];\n",
      "\tmov.u32 \t%r3, %ntid.x;\n",
      "\tmul.wide.u32 \t%rd2, %r3, %r2;\n",
      "\tmov.u32 \t%r1, %tid.x;\n",
      "\tadd.s32 \t%r4, %r1, 1;\n",
      "\tcvt.u64.u32 \t%rd26, %r4;\n",
      "\tadd.s64 \t%rd27, %rd2, %rd26;\n",
      "\tmul.lo.s64 \t%rd28, %rd25, %rd24;\n",
      "\tsetp.gt.s64 \t%p1, %rd27, %rd28;\n",
      "\t@%p1 bra \tLBB0_4;\n",
      "// %bb.1:                               // %L42.i\n",
      "\tld.param.u64 \t%rd23, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_5+16];\n",
      "\tld.param.u64 \t%rd20, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_4+16];\n",
      "\tld.param.u64 \t%rd17, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_3+16];\n",
      "\tld.param.u64 \t%rd14, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_2+16];\n",
      "\tld.param.u64 \t%rd11, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_1+16];\n",
      "\tld.param.u64 \t%rd8, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_0+16];\n",
      "\tld.param.u64 \t%rd1, [_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE_param_6+16];\n",
      "\tcvt.u64.u32 \t%rd30, %r1;\n",
      "\tadd.s64 \t%rd31, %rd2, %rd30;\n",
      "\tadd.s64 \t%rd3, %rd31, -1;\n",
      "\tshl.b64 \t%rd32, %rd3, 2;\n",
      "\tadd.s64 \t%rd33, %rd8, %rd32;\n",
      "\tld.global.f32 \t%f27, [%rd33+4];\n",
      "\tadd.s64 \t%rd34, %rd11, %rd32;\n",
      "\tld.global.f32 \t%f26, [%rd34+4];\n",
      "\tadd.s64 \t%rd35, %rd14, %rd32;\n",
      "\tld.global.f32 \t%f25, [%rd35+4];\n",
      "\tadd.s64 \t%rd36, %rd17, %rd32;\n",
      "\tld.global.f32 \t%f24, [%rd36+4];\n",
      "\tadd.s64 \t%rd37, %rd20, %rd32;\n",
      "\tld.global.f32 \t%f23, [%rd37+4];\n",
      "\tadd.s64 \t%rd38, %rd23, %rd32;\n",
      "\tld.global.f32 \t%f22, [%rd38+4];\n",
      "\tmov.u64 \t%rd41, 33;\n",
      "LBB0_2:                                 // %L181.i\n",
      "                                        // =>This Inner Loop Header: Depth=1\n",
      "\tfma.rn.f32 \t%f13, %f27, %f26, %f25;\n",
      "\tfma.rn.f32 \t%f14, %f24, %f23, %f22;\n",
      "\tfma.rn.f32 \t%f15, %f25, %f26, %f27;\n",
      "\tfma.rn.f32 \t%f16, %f22, %f23, %f24;\n",
      "\tfma.rn.f32 \t%f23, %f27, %f25, %f23;\n",
      "\tfma.rn.f32 \t%f22, %f26, %f24, %f22;\n",
      "\tadd.s64 \t%rd41, %rd41, -1;\n",
      "\tsetp.ne.s64 \t%p2, %rd41, 0;\n",
      "\tmov.f32 \t%f24, %f16;\n",
      "\tmov.f32 \t%f25, %f15;\n",
      "\tmov.f32 \t%f26, %f14;\n",
      "\tmov.f32 \t%f27, %f13;\n",
      "\t@%p2 bra \tLBB0_2;\n",
      "// %bb.3:                               // %L199.L204_crit_edge.i\n",
      "\tfma.rn.f32 \t%f19, %f13, %f14, %f15;\n",
      "\tfma.rn.f32 \t%f20, %f16, %f23, %f22;\n",
      "\tfma.rn.f32 \t%f21, %f19, %f20, %f20;\n",
      "\tadd.s64 \t%rd40, %rd1, %rd32;\n",
      "\tst.global.f32 \t[%rd40+4], %f21;\n",
      "LBB0_4:                                 // %_Z26julia_kernel_201_fma_1156313CuDeviceArrayI7Float32Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EES_IS0_Li2ELi1EE.inner.exit\n",
      "\tret;\n",
      "                                        // -- End function\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "CUDA.@device_code_ptx cudapeakflops(Float32) # not unrolled :( if you want to unroll a loop by hand to be 100% sure it does what you want, take a look at Base.Cartesian.@nexprs, a neat macro!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plugging a CUDA-unaware package into our kernel\n",
    "\n",
    "Remember MultiFloats? They are very nice. But the package doesn't seem to have `fma` implemented :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MultiFloats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fma(Float64x2(1.0), Float64x2(1.0), Float64x2(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a generic `fma` function for MultiFloats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fma (generic function with 33 methods)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: fma\n",
    "\n",
    "fma(x::MultiFloat, y::MultiFloat, z::MultiFloat) = x * y + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Test\n",
    "\n",
    "@test fma(Float64x2(2.0), Float64x2(3.0), Float64x2(4.0)) == Float64x2(10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our cudapeakflops function with Float64x{N}!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124.36 GMultiFlx2ops (Giga MultiFloat{Float64,2} operations per second)\n"
     ]
    }
   ],
   "source": [
    "println(cudapeakflops(Float64x2), \" GMultiFlx2ops (Giga MultiFloat{Float64,2} operations per second)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One MultiFloat fma is 2 MultiFloat operations, but how many flop is a MultiFloat-op? Let's find out by using the GFlops package -- in another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We counted an fma as 2 flops, but for Float64x2 it is actually 36 flops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2491.92 GFlops for Float64x2, or 62.3% of GPU peak\n"
     ]
    }
   ],
   "source": [
    "println(138.44 / 2 * 36, \" GFlops for Float64x2, or \", round((138.44 / 2 * 36) / 4000 * 100, digits=2), \"% of GPU peak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another instance of \"unexpected\" composability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×100 Matrix{MultiFloat{Float64, 8}}:\n",
       " -9.09793e-115   5.40507e-115   2.88325e-115  …  -2.01245e-114   1.45208e-114\n",
       "  1.38352e-114   2.33765e-114  -8.85281e-115     -7.29677e-115  -4.08252e-115\n",
       "  1.95259e-115  -6.36662e-115  -1.01853e-115     -2.39703e-115   5.68613e-116\n",
       "  5.23776e-116   2.08698e-115  -1.87998e-114     -8.64719e-115  -1.30447e-115\n",
       " -1.72312e-115  -5.42238e-115  -6.09206e-115      6.31143e-116  -6.16511e-115\n",
       " -1.67153e-115   5.44232e-115   1.22109e-114  …   1.06451e-114  -8.37738e-117\n",
       "  6.19677e-116   1.71323e-114  -1.04796e-114     -1.55155e-114  -9.65254e-115\n",
       "  5.23525e-115  -4.29548e-115   8.64368e-116     -3.4625e-115   -4.61405e-115\n",
       "  4.30137e-115  -1.50597e-115   2.63562e-115      5.51376e-115  -4.17732e-115\n",
       " -4.5678e-116   -2.45388e-115   6.11103e-116      3.09443e-115   5.32819e-115\n",
       "  1.74628e-114  -3.53623e-115   4.45584e-115  …  -6.63469e-115   4.08957e-115\n",
       " -1.02859e-114   1.22689e-114   8.70212e-115     -7.99985e-115   2.67055e-115\n",
       "  1.4013e-114    2.52936e-115   1.47628e-115     -1.53943e-114  -5.84176e-115\n",
       "  ⋮                                           ⋱                 \n",
       "  1.07196e-114   1.91478e-115   6.47645e-115     -7.86219e-115   9.5877e-115\n",
       "  8.80832e-115  -3.27397e-116   3.93046e-115     -2.26964e-115  -2.9946e-115\n",
       "  6.57162e-115   6.22802e-115   4.06269e-115  …   5.76319e-117   9.98311e-115\n",
       " -8.70032e-116  -4.34661e-115   1.08688e-114      3.01595e-115   1.14673e-114\n",
       " -2.17163e-115   8.09192e-116   2.53175e-115      2.97903e-116   1.84385e-115\n",
       " -4.10402e-115   4.3046e-115    4.15779e-115      3.14015e-115   1.85197e-115\n",
       "  1.22565e-114  -3.30028e-115   2.00204e-114     -9.25457e-116  -4.72484e-115\n",
       "  7.75601e-115  -4.97875e-115  -8.6681e-115   …   3.20863e-115   2.54042e-116\n",
       "  1.08241e-114   2.74255e-115   7.06446e-116     -4.74516e-116  -2.4713e-115\n",
       "  3.99692e-115   1.69142e-115  -5.3233e-115       1.01575e-115  -1.23007e-115\n",
       " -5.26121e-115  -8.79512e-116  -4.29139e-115      1.76821e-114  -1.99566e-115\n",
       "  4.32482e-115   3.48378e-115  -2.65071e-115     -4.34536e-116   4.12316e-115"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(Float64x8, 100, 100)\n",
    "B = rand(Float64x8, 100, 100)\n",
    "\n",
    "# Do note that there is *some* precision loss, eps(Float64x8) == 5.9091063153828709e-126; it's tabulated in the MultiFloats.jl readme!\n",
    "A * B - Array(CuArray(A) * CuArray(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.911391 seconds (2.84 M allocations: 109.230 MiB, 0.79% gc time, 10.90% compilation time)\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra: mul!\n",
    "A_d = CuArray(rand(Float64x8, 1000, 1000))\n",
    "B_d = CuArray(rand(Float64x8, 1000, 1000))\n",
    "C_d = similar(A_d)\n",
    "\n",
    "@time CUDA.@sync mul!(C_d, A_d, B_d);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we go further? Things started breaking down for me after double x10; but do note that we're there in roughly $2 \\times 10^3$ flop per multiflop areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiFloats.use_standard_multifloat_arithmetic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Matrix{Float64x{10}}:\n",
       " 0.581641   0.944599  0.144027   …  0.704115     0.857323  0.618346\n",
       " 0.153017   0.661059  0.43499       0.0191004    0.524941  0.445183\n",
       " 0.829292   0.398566  0.869847      0.000906595  0.711319  0.201016\n",
       " 0.446489   0.567816  0.851343      0.530009     0.194725  0.192469\n",
       " 0.25493    0.828104  0.691898      0.9868       0.861545  0.0556866\n",
       " 0.0812726  0.772254  0.581123   …  0.788337     0.784821  0.17692\n",
       " 0.242289   0.896374  0.0804072     0.786013     0.277007  0.0382205\n",
       " 0.380922   0.998832  0.608276      0.916063     0.96271   0.330946\n",
       " 0.288848   0.187473  0.285251      0.590867     0.126445  0.954407\n",
       " 0.386847   0.614478  0.112511      0.959511     0.617191  0.953837"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, B = rand(Float64x{10}, 10, 10), rand(Float64x{10}, 10, 10) # exercise for the reader :)\n",
    "A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
